{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nmicroTVM with TFLite Models\n===========================\n**Author**: `Tom Gall <https://github.com/tom-gall>`_\n\nThis tutorial is an introduction to working with microTVM and a TFLite\nmodel with Relay.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>If you want to run this tutorial on the microTVM Reference VM, download the Jupyter\n    notebook using the link at the bottom of this page and save it into the TVM directory. Then:\n\n    #. Login to the reference VM with a modified ``vagrant ssh`` command:\n\n        ``$ vagrant ssh -- -L8888:localhost:8888``\n\n    #. Install jupyter:  ``pip install jupyterlab``\n    #. ``cd`` to the TVM directory.\n    #. Install tflite: poetry install -E importer-tflite\n    #. Launch Jupyter Notebook: ``jupyter notebook``\n    #. Copy the localhost URL displayed, and paste it into your browser.\n    #. Navigate to saved Jupyter Notebook (``.ipynb`` file).</p></div>\n\n\nSetup\n-----\n\nInstall TFLite\n^^^^^^^^^^^^^^\n\nTo get started, TFLite package needs to be installed as prerequisite. You can do this in two ways:\n\n1. Install tflite with ``pip``\n\n    .. code-block:: bash\n\n      pip install tflite=2.1.0 --user\n\n2. Generate the TFLite package yourself. The steps are the following:\n\n    Get the flatc compiler.\n    Please refer to https://github.com/google/flatbuffers for details\n    and make sure it is properly installed.\n\n    .. code-block:: bash\n\n      flatc --version\n\n    Get the TFLite schema.\n\n    .. code-block:: bash\n\n      wget https://raw.githubusercontent.com/tensorflow/tensorflow/r1.13/tensorflow/lite/schema/schema.fbs\n\n    Generate TFLite package.\n\n    .. code-block:: bash\n\n      flatc --python schema.fbs\n\n    Add the current folder (which contains generated tflite module) to PYTHONPATH.\n\n    .. code-block:: bash\n\n      export PYTHONPATH=${PYTHONPATH:+$PYTHONPATH:}$(pwd)\n\nTo validate that the TFLite package was installed successfully, ``python -c \"import tflite\"``\n\nInstall Zephyr (physical hardware only)\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nWhen running this tutorial with a host simulation (the default), you can use the host ``gcc`` to\nbuild a firmware image that simulates the device. When compiling to run on physical hardware, you\nneed to install a *toolchain* plus some target-specific dependencies. microTVM allows you to\nsupply any compiler and runtime that can launch the TVM RPC server, but to get started, this\ntutorial relies on the Zephyr RTOS to provide these pieces.\n\nYou can install Zephyr by following the\n`Installation Instructions <https://docs.zephyrproject.org/latest/getting_started/index.html>`_.\n\nAside: Recreating your own Pre-Trained TFLite model\n The tutorial downloads a pretrained TFLite model. When working with microcontrollers\n you need to be mindful these are highly resource constrained devices as such standard\n models like MobileNet may not fit into their modest memory.\n\n For this tutorial, we'll make use of one of the TF Micro example models.\n\n If you wish to replicate the training steps see:\n https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world/train\n\n   .. note::\n\n     If you accidentally download the example pretrained model from:\n\n     ``wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/hello_world_2020_04_13.zip``\n\n     this will fail due to an unimplemented opcode (114)\n\nLoad and prepare the Pre-Trained Model\n--------------------------------------\n\nLoad the pretrained TFLite model from a file in your current\ndirectory into a buffer\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport logging\n\nimport tvm\nimport tvm.micro as micro\nfrom tvm.contrib.download import download_testdata\nfrom tvm.contrib import graph_executor, utils\nfrom tvm import relay\n\nmodel_url = \"https://people.linaro.org/~tom.gall/sine_model.tflite\"\nmodel_file = \"sine_model.tflite\"\nmodel_path = download_testdata(model_url, model_file, module=\"data\")\n\ntflite_model_buf = open(model_path, \"rb\").read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the buffer, transform into a tflite model python object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import tflite\n\n    tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\nexcept AttributeError:\n    import tflite.Model\n\n    tflite_model = tflite.Model.Model.GetRootAsModel(tflite_model_buf, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print out the version of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "version = tflite_model.Version()\nprint(\"Model Version: \" + str(version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse the python model object to convert it into a relay module\nand weights.\nIt is important to note that the input tensor name must match what\nis contained in the model.\n\nIf you are unsure what that might be, this can be discovered by using\nthe ``visualize.py`` script within the Tensorflow project.\nSee `How do I inspect a .tflite file? <https://www.tensorflow.org/lite/guide/faq>`_\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_tensor = \"dense_4_input\"\ninput_shape = (1,)\ninput_dtype = \"float32\"\n\nmod, params = relay.frontend.from_tflite(\n    tflite_model, shape_dict={input_tensor: input_shape}, dtype_dict={input_tensor: input_dtype}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the target\n-------------------\n\nNow we create a build config for relay. turning off two options\nand then calling relay.build which will result in a C source\nfile. When running on a simulated target, choose \"host\" below:\nTARGET = tvm.target.target.micro(\"host\")\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %%\n# Compiling for physical hardware\n#  When running on physical hardware, choose a target and a board that\n#  describe the hardware. The STM32F746 Nucleo target and board is chosen in\n#  this commented code. Another option would be to choose the same target but\n#  the STM32F746 Discovery board instead. The disco board has the same\n#  microcontroller as the Nucleo board but a couple of wirings and configs\n#  differ, so it's necessary to select the \"stm32f746g_disco\" board below.\n#\n#  .. code-block:: python\n#\nTARGET = tvm.target.target.micro(\"host\")\n# BOARD = \"nucleo_f746zg\" # or \"stm32f746g_disco\"\nBOARD = \"qemu_x86\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, compile the model for the target:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tvm.transform.PassContext(\n    opt_level=3, config={\"tir.disable_vectorize\": True}, disabled_pass=[\"FuseOps\", \"AlterOpLayout\"]\n):\n    graph, c_mod, c_params = relay.build(mod, target=TARGET, params=params)\n\n\n# %%\n# Compiling for a simulated device\n# --------------------------------\n#\n# First, compile a static microTVM runtime for the targeted device. In this case, the host simulated\n# device is used.\ncompiler = tvm.micro.DefaultCompiler(target=TARGET)\nopts = tvm.micro.default_options(\n    os.path.join(tvm.micro.get_standalone_crt_dir(), \"template\", \"host\")\n)\n\n# %%\n# Compiling for physical hardware\n#  For physical hardware, comment out the previous section and use this compiler definition instead.\n#\n#  .. code-block:: python\n#\n#     import subprocess\n#     from tvm.micro.contrib import zephyr\n#\n#     repo_root = subprocess.check_output([\"git\", \"rev-parse\", \"--show-toplevel\"], encoding='utf-8').strip()\n#     project_dir = f\"{repo_root}/tests/micro/qemu/zephyr-runtime\"\n#     compiler = zephyr.ZephyrCompiler(\n#         project_dir=project_dir,\n#         board=BOARD if \"stm32f746\" in str(TARGET) else \"qemu_x86\",\n#         zephyr_toolchain_variant=\"zephyr\",\n#     )\n#\n#     opts = tvm.micro.default_options(f\"{project_dir}/crt\")\n#\n# enable printing memory usage statistics of the runtime image\n# generated by Zephyr compiler for the physical hardware\n# logging.basicConfig(level=\"INFO\")\n\nworkspace = tvm.micro.Workspace()\nmicro_binary = tvm.micro.build_static_runtime(\n    workspace,\n    compiler,\n    c_mod,\n    opts,\n    # Use the microTVM memory manager. If, in your main.cc, you change TVMPlatformMemoryAllocate and\n    # TVMPlatformMemoryFree to use e.g. malloc() and free(), you can omit this extra library.\n    extra_libs=[tvm.micro.get_standalone_crt_lib(\"memory\")],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, establish a session with the simulated device and run the\ncomputation. The `with session` line would typically flash an attached\nmicrocontroller, but in this tutorial, it simply launches a subprocess\nto stand in for an attached microcontroller.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flasher = compiler.flasher()\nwith tvm.micro.Session(binary=micro_binary, flasher=flasher) as session:\n    graph_mod = tvm.micro.create_local_graph_executor(\n        graph, session.get_system_lib(), session.device\n    )\n\n    # Set the model parameters using the lowered parameters produced by `relay.build`.\n    graph_mod.set_input(**c_params)\n\n    # The model consumes a single float32 value and returns a predicted sine value.  To pass the\n    # input value we construct a tvm.nd.array object with a single contrived number as input. For\n    # this model values of 0 to 2Pi are acceptable.\n    graph_mod.set_input(input_tensor, tvm.nd.array(np.array([0.5], dtype=\"float32\")))\n    graph_mod.run()\n\n    tvm_output = graph_mod.get_output(0).asnumpy()\n    print(\"result is: \" + str(tvm_output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}