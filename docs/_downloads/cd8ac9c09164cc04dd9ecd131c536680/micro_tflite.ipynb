{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMicro TVM with TFLite Models\n============================\n**Author**: `Tom Gall <https://github.com/tom-gall>`_\n\nThis tutorial is an introduction to working with MicroTVM and a TFLite \nmodel with Relay.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# %%\n# Setup\n# -----\n#\n# To get started, TFLite package needs to be installed as prerequisite.\n#\n# install tflite\n#\n# .. code-block:: bash\n#\n#   pip install tflite=2.1.0 --user\n#\n# or you could generate TFLite package yourself. The steps are the following:\n#\n#   Get the flatc compiler.\n#   Please refer to https://github.com/google/flatbuffers for details\n#   and make sure it is properly installed.\n#\n# .. code-block:: bash\n#\n#   flatc --version\n#\n# Get the TFLite schema.\n#\n# .. code-block:: bash\n#\n#   wget https://raw.githubusercontent.com/tensorflow/tensorflow/r1.13/tensorflow/lite/schema/schema.fbs\n#\n# Generate TFLite package.\n#\n# .. code-block:: bash\n#\n#   flatc --python schema.fbs\n#\n# Add the current folder (which contains generated tflite module) to PYTHONPATH.\n#\n# .. code-block:: bash\n#\n#   export PYTHONPATH=${PYTHONPATH:+$PYTHONPATH:}$(pwd)\n#\n# To validate that the TFLite package was installed successfully, ``python -c \"import tflite\"``\n#\n# CMSIS needs to be downloaded and the CMSIS_ST_PATH environment variable setup\n# This tutorial only supports the STM32F7xx series of boards.\n# Download from : https://www.st.com/en/embedded-software/stm32cubef7.html\n# After you've expanded the zip file\n#\n# .. code-block:: bash\n#\n#   export CMSIS_ST_PATH=/path/to/STM32Cube_FW_F7_V1.16.0/Drivers/CMSIS\n\n# %%\n# Recreating your own Pre-Trained TFLite model\n# --------------------------------------------\n#\n# The tutorial downloads a pretrained TFLite model. When working with microcontrollers\n# you need to be mindful these are highly resource constrained devices as such standard\n# models like MobileNet may not fit into their modest memory.\n#\n# For this tutorial, we'll make use of one of the TF Micro example models.\n#\n# If you wish to replicate the training steps see:\n# https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world/train\n#\n#   .. note::\n#\n#     If you accidentally download the example pretrained model from:\n#     wget https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/hello_world_2020_04_13.zip\n#     this will fail due to an unimplemented opcode (114)\n\nimport os\nimport numpy as np\nimport tvm\nimport tvm.micro as micro\nfrom tvm.contrib.download import download_testdata\nfrom tvm.contrib import graph_runtime, util\nfrom tvm import relay\n\n# %%\n# Load and prepare the Pre-Trained Model\n# --------------------------------------\n#\n# Load the pretrained TFLite model from a file in your current\n# directory into a buffer\n\nmodel_url = 'https://people.linaro.org/~tom.gall/sine_model.tflite'\nmodel_file = 'sine_model.tflite'\nmodel_path = download_testdata(model_url, model_file, module='data')\n\ntflite_model_buf = open(model_path, \"rb\").read()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the buffer, transform into a tflite model python object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    import tflite\n    tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\nexcept AttributeError:\n    import tflite.Model\n    tflite_model = tflite.Model.Model.GetRootAsModel(tflite_model_buf, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print out the version of the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "version = tflite_model.Version()\nprint (\"Model Version: \" + str(version))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parse the python model object to convert it into a relay module\nand weights.\nIt is important to note that the input tensor name must match what\nis contained in the model.\n\nIf you are unsure what that might be, this can be discovered by using\nthe visualize.py script within the Tensorflow project.\nSee : How do I inspect a .tflite file? `<https://www.tensorflow.org/lite/guide/faq>`_\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_tensor = \"dense_4_input\"\ninput_shape = (1,)\ninput_dtype = \"float32\"\n\nmod, params = relay.frontend.from_tflite(tflite_model,\n                                         shape_dict={input_tensor: input_shape},\n                                         dtype_dict={input_tensor: input_dtype})\n\n# %%\n# Running on device\n# ----------------------------------------------\n#\n# Setup the device config which is what will be used to communicate\n# with the microcontroller (a STM32F746 Discovery board)\nTARGET = 'c --system-lib  --runtime=c'\ndev_config = micro.device.arm.stm32f746xx.generate_config(\"127.0.0.1\", 6666)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next with the dev_config, we establish a micro session and create\na context\n\n.. code-block:: python\n\n  with micro.Session(dev_config) as sess:\n      ctx = tvm.micro_dev(0)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a build config for relay. turning off two options\nand then calling relay.build which will result in a C source\nfile.\n\n.. code-block:: python\n\n  with tvm.transform.PassContext(opt_level=3, config={'tir.disable_vectorize': True},disabled_pass=['FuseOps']):\n      graph, c_mod, params = relay.build(mod, target=TARGET, params=params)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the c_mod that is the handle to our C source code, we create a\nmicro module, followed by a compiled object which behind the scenes\nis linked to the microTVM runtime for running on the target board\n\n.. code-block:: python\n\n  micro_mod = micro.create_micro_mod(c_mod, dev_config)\n  mod = graph_runtime.create(graph, micro_mod, ctx)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pass the weights to get ready to perform inference\n\n.. code-block:: python\n\n  mod.set_input(**params)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model consumes a single float32 value and returns a predicted\nsine value.\nTo pass the input value we construct a tvm.nd.array object\nwith a single contrived number as input. For this model values of\n0 to 2Pi are acceptable.\n\n.. code-block:: python\n\n  mod.set_input(input_tensor, tvm.nd.array(np.array([0.5], dtype=\"float32\")))\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the model on device\n\n.. code-block:: python\n\n  mod.run()\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get output from the run and print\n\n.. code-block:: python\n\n  tvm_output = mod.get_output(0).asnumpy()\n  print(\"result is: \"+str(tvm_output))\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}