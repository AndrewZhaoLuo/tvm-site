.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py:


.. _auto-scheduler-conv-gpu:

Auto-scheduling a Convolution Layer for GPU
===========================================
**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_

This is a tutorial on how to use the auto-scheduler for GPUs.

Different from the template-based :ref:`autotvm <tutorials-autotvm-sec>` which relies on
manual templates to define the search space, the auto-scheduler does not require any templates.
Users only need to write the computation declaration without any schedule commands or templates.
The auto-scheduler can automatically generate a large search space and
find a good schedule in the space.

We use a convolution layer as an example in this tutorial.

Note that this tutorial will not run on Windows or recent versions of macOS. To
get it to run, you will need to wrap the body of this tutorial in a :code:`if
__name__ == "__main__":` block.


.. code-block:: default


    import os

    import numpy as np
    import tvm
    from tvm import te, auto_scheduler, topi
    from tvm.topi.testing import conv2d_nchw_python







Define the computation
^^^^^^^^^^^^^^^^^^^^^^
To begin with, let us define the computation of a convolution layer.
The function should return the list of input/output tensors.
From these tensors, the auto-scheduler can get the whole computational graph.


.. code-block:: default



    @auto_scheduler.register_workload
    def conv2d_layer(N, H, W, CO, CI, KH, KW, stride, padding):
        data = te.placeholder((N, CI, H, W), name="data")
        kernel = te.placeholder((CO, CI, KH, KW), name="kernel")
        bias = te.placeholder((1, CO, 1, 1), name="bias")
        conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype="float32")
        out = topi.nn.relu(conv + bias)
        return [data, kernel, bias, out]








Create the search task
^^^^^^^^^^^^^^^^^^^^^^
We then create a search task for the last convolution layer in the resnet.


.. code-block:: default


    target = tvm.target.Target("cuda")

    # Use the last layer in ResNet-50
    N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)
    task = auto_scheduler.SearchTask(
        func=conv2d_layer, args=(N, H, W, CO, CI, KH, KW, strides, padding), target=target
    )

    # Inspect the computational graph
    print("Computational DAG:")
    print(task.compute_dag)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Computational DAG:
    data = PLACEHOLDER [1, 512, 7, 7]
    pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i2 >= 1) && (i2 < 8)) && (i3 >= 1)) && (i3 < 8)), data[i0, i1, (i2 - 1), (i3 - 1)], 0f)
    kernel = PLACEHOLDER [512, 512, 3, 3]
    compute(nn, ff, yy, xx) += (pad_temp[nn, rc, (yy + ry), (xx + rx)]*kernel[ff, rc, ry, rx])
    bias = PLACEHOLDER [1, 512, 1, 1]
    T_add(ax0, ax1, ax2, ax3) = (compute[ax0, ax1, ax2, ax3] + bias[ax0, ax1, 0, 0])
    compute(i0, i1, i2, i3) = max(T_add[i0, i1, i2, i3], 0f)




Next, we set parameters for the auto-scheduler. These parameters
mainly specify how we do the measurement during the search.

* :code:`measure_ctx` launches a different process for measurement to
  provide isolation. It can protect the master process from GPU crashes
  during measurement and avoid other runtime conflicts.
* :code:`min_repeat_ms` defines the minimum duration of one "repeat" in every measurement.
  This can warmup the GPU, which is necessary to get accurate measurement results.
  Typically, we recommend a value >= 300 ms.
* :code:`num_measure_trials` is the number of measurement trials we can use during the search.
  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a
  good value for the search to converge. You can do more trials according to your time budget.
* In addition, we use :code:`RecordToFile` to dump measurement records into a file `conv2d.json`.
  The measurement records can be used to query the history best, resume the search,
  and do more analyses later.
* see :any:`auto_scheduler.TuningOptions`,
  :any:`auto_scheduler.LocalRPCMeasureContext` for more parameters.


.. code-block:: default


    log_file = "conv2d.json"
    measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=10,  # change this to 1000 to achieve the best performance
        runner=measure_ctx.runner,
        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
        verbose=2,
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Get devices for measurement successfully!



Run the search
^^^^^^^^^^^^^^
Now we get all inputs ready. Pretty simple, isn't it?
We can kick off the search and let the auto-scheduler do its magic.
After some measurement trials, we can load the best schedule from the log
file and apply it.


.. code-block:: default


    # Run auto-tuning (search)
    task.tune(tune_option)
    # Apply the best schedule
    sch, args = task.apply_best(log_file)

    # Kill the measurement process
    del measure_ctx





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none





We can lower the schedule to see the IR after auto-scheduling.
The auto-scheduler correctly performs optimizations including multi-level tiling,
cooperative fetching, unrolling and operator fusion.


.. code-block:: default


    print("Lowered TIR:")
    print(tvm.lower(sch, args, simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Lowered TIR:
    primfn(data_1: handle, kernel_1: handle, bias_1: handle, compute_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {compute: Buffer(compute_2: Pointer(float32), float32, [1, 512, 7, 7], []),
                 bias: Buffer(bias_2: Pointer(float32), float32, [1, 512, 1, 1], []),
                 kernel: Buffer(kernel_2: Pointer(float32), float32, [512, 512, 3, 3], []),
                 data: Buffer(data_2: Pointer(float32), float32, [1, 512, 7, 7], [])}
      buffer_map = {data_1: data, kernel_1: kernel, bias_1: bias, compute_1: compute} {
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 112;
      attr [compute_3: Pointer(float32)] "storage_scope" = "local";
      allocate(compute_3, float32, [4]);
      attr [pad_temp.shared: Pointer(float32)] "storage_scope" = "shared";
      allocate(pad_temp.shared, float32, [54]);
      attr [kernel.shared: Pointer(float32)] "storage_scope" = "shared";
      allocate(kernel.shared, float32, [576]);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56 {
        compute_3[0] = 0f32
        compute_3[2] = 0f32
        compute_3[1] = 0f32
        compute_3[3] = 0f32
        for (rc.outer.outer: int32, 0, 256) {
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          if @tir.likely((threadIdx.x_1 < 54), dtype=bool) {
            pad_temp.shared[threadIdx.x_1] = @tir.if_then_else(((((1 <= (floordiv(floormod(threadIdx.x_1, 27), 9) + floormod(blockIdx.x, 7))) && ((floordiv(floormod(threadIdx.x_1, 27), 9) + floormod(blockIdx.x, 7)) < 8)) && (1 <= floormod(threadIdx.x_1, 9))) && (floormod(threadIdx.x_1, 9) < 8)), (float32*)data_2[((((((rc.outer.outer*98) + (floordiv(threadIdx.x_1, 27)*49)) + (floordiv(floormod(threadIdx.x_1, 27), 9)*7)) + (floormod(blockIdx.x, 7)*7)) + floormod(threadIdx.x_1, 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[threadIdx.x_2] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv(threadIdx.x_2, 18)*4608)) + (rc.outer.outer*18)) + floormod(threadIdx.x_2, 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 56)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 56), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 2), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 112)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 112), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 4), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 168)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 168), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 6), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 224)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 224), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 8), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 280)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 280), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 10), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 336)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 336), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 12), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 392)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 392), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 14), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 448)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 448), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 16), 18))]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          kernel.shared[(threadIdx.x_2 + 504)] = (float32*)kernel_2[(((((floordiv(blockIdx.x, 7)*147456) + (floordiv(threadIdx.x_2, 18)*4608)) + (rc.outer.outer*18)) + floormod(threadIdx.x_2, 18)) + 129024)]
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 56;
          if @tir.likely((threadIdx.x_2 < 16), dtype=bool) {
            kernel.shared[(threadIdx.x_2 + 560)] = (float32*)kernel_2[((((floordiv(blockIdx.x, 7)*147456) + (floordiv((threadIdx.x_2 + 560), 18)*4608)) + (rc.outer.outer*18)) + floormod((threadIdx.x_2 + 2), 18))]
          }
          for (rc.outer.inner: int32, 0, 2) {
            for (rx.outer.inner: int32, 0, 3) {
              compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7))]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner)]))
              compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7))]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 288)]))
              compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 9)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 3)]))
              compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 9)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 291)]))
              compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 18)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 6)]))
              compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 18)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 294)]))
              compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7))]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 18)]))
              compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7))]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 306)]))
              compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 9)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 21)]))
              compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 9)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 309)]))
              compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 18)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 24)]))
              compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[((((rc.outer.inner*27) + rx.outer.inner) + floormod(threadIdx.x, 7)) + 18)]*(float32*)kernel.shared[((((floordiv(threadIdx.x, 7)*36) + (rc.outer.inner*9)) + rx.outer.inner) + 312)]))
            }
          }
        }
        for (i1.inner: int32, 0, 2) {
          compute_2[(((((floordiv(blockIdx.x, 7)*1568) + (floordiv(threadIdx.x, 7)*98)) + (i1.inner*49)) + (floormod(blockIdx.x, 7)*7)) + floormod(threadIdx.x, 7))] = max(((float32*)compute_3[i1.inner] + (float32*)bias_2[(((floordiv(blockIdx.x, 7)*32) + (floordiv(threadIdx.x, 7)*2)) + i1.inner)]), 0f32)
          compute_2[((((((floordiv(blockIdx.x, 7)*1568) + (floordiv(threadIdx.x, 7)*98)) + (i1.inner*49)) + (floormod(blockIdx.x, 7)*7)) + floormod(threadIdx.x, 7)) + 784)] = max(((float32*)compute_3[(i1.inner + 2)] + (float32*)bias_2[((((floordiv(blockIdx.x, 7)*32) + (floordiv(threadIdx.x, 7)*2)) + i1.inner) + 16)]), 0f32)
        }
      }
    }





Check correctness and evaluate performance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We build the binary and check its correctness and performance.


.. code-block:: default


    func = tvm.build(sch, args, target)

    # Check correctness
    data_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)
    weight_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)
    bias_np = np.random.uniform(size=(1, CO, 1, 1)).astype(np.float32)
    conv_np = conv2d_nchw_python(data_np, weight_np, strides, padding)
    out_np = np.maximum(conv_np + bias_np, 0.0)

    ctx = tvm.gpu()
    data_tvm = tvm.nd.array(data_np, ctx=ctx)
    weight_tvm = tvm.nd.array(weight_np, ctx=ctx)
    bias_tvm = tvm.nd.array(bias_np, ctx=ctx)
    out_tvm = tvm.nd.empty(out_np.shape, ctx=ctx)
    func(data_tvm, weight_tvm, bias_tvm, out_tvm)

    # Check results
    np.testing.assert_allclose(out_np, out_tvm.asnumpy(), rtol=1e-3)

    # Evaluate execution time
    evaluator = func.time_evaluator(func.entry_name, ctx, min_repeat_ms=500)
    print(
        "Execution time of this operator: %.3f ms"
        % (np.median(evaluator(data_tvm, weight_tvm, bias_tvm, out_tvm).results) * 1000)
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Execution time of this operator: 0.289 ms



Using the record file
^^^^^^^^^^^^^^^^^^^^^
During the search, all measurement records are dumped into the record
file "conv2d.json". The measurement records can be used to re-apply search results,
resume the search, and perform other analyses.

Here is an example where we load the best schedule from a file,
print the equivalent python schedule API and CUDA source code.
They can be used for debugging and learning the behavior of the auto-scheduler.


.. code-block:: default


    print("Equivalent python schedule:")
    print(task.print_best(log_file, print_mode="schedule"))

    print("CUDA source code:")
    print(task.print_best(log_file, print_mode="cuda"))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Equivalent python schedule:
    pad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)
    compute_nn, compute_ff, compute_yy, compute_xx, compute_rc, compute_ry, compute_rx = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
    T_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
    compute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
    s[T_add].compute_inline()
    compute_nn_o_i, compute_nn_i = s[compute].split(compute_nn, factor=1)
    compute_nn_o_o_i, compute_nn_o_i = s[compute].split(compute_nn_o_i, factor=1)
    compute_nn_o_o_o_i, compute_nn_o_o_i = s[compute].split(compute_nn_o_o_i, factor=1)
    compute_nn_o_o_o_o, compute_nn_o_o_o_i = s[compute].split(compute_nn_o_o_o_i, factor=1)
    compute_ff_o_i, compute_ff_i = s[compute].split(compute_ff, factor=1)
    compute_ff_o_o_i, compute_ff_o_i = s[compute].split(compute_ff_o_i, factor=2)
    compute_ff_o_o_o_i, compute_ff_o_o_i = s[compute].split(compute_ff_o_o_i, factor=8)
    compute_ff_o_o_o_o, compute_ff_o_o_o_i = s[compute].split(compute_ff_o_o_o_i, factor=2)
    compute_yy_o_i, compute_yy_i = s[compute].split(compute_yy, factor=1)
    compute_yy_o_o_i, compute_yy_o_i = s[compute].split(compute_yy_o_i, factor=1)
    compute_yy_o_o_o_i, compute_yy_o_o_i = s[compute].split(compute_yy_o_o_i, factor=1)
    compute_yy_o_o_o_o, compute_yy_o_o_o_i = s[compute].split(compute_yy_o_o_o_i, factor=1)
    compute_xx_o_i, compute_xx_i = s[compute].split(compute_xx, factor=1)
    compute_xx_o_o_i, compute_xx_o_i = s[compute].split(compute_xx_o_i, factor=1)
    compute_xx_o_o_o_i, compute_xx_o_o_i = s[compute].split(compute_xx_o_o_i, factor=7)
    compute_xx_o_o_o_o, compute_xx_o_o_o_i = s[compute].split(compute_xx_o_o_o_i, factor=1)
    compute_rc_o_i, compute_rc_i = s[compute].split(compute_rc, factor=1)
    compute_rc_o_o, compute_rc_o_i = s[compute].split(compute_rc_o_i, factor=2)
    compute_ry_o_i, compute_ry_i = s[compute].split(compute_ry, factor=3)
    compute_ry_o_o, compute_ry_o_i = s[compute].split(compute_ry_o_i, factor=1)
    compute_rx_o_i, compute_rx_i = s[compute].split(compute_rx, factor=1)
    compute_rx_o_o, compute_rx_o_i = s[compute].split(compute_rx_o_i, factor=3)
    s[compute].reorder(compute_nn_o_o_o_o, compute_ff_o_o_o_o, compute_yy_o_o_o_o, compute_xx_o_o_o_o, compute_nn_o_o_o_i, compute_ff_o_o_o_i, compute_yy_o_o_o_i, compute_xx_o_o_o_i, compute_nn_o_o_i, compute_ff_o_o_i, compute_yy_o_o_i, compute_xx_o_o_i, compute_rc_o_o, compute_ry_o_o, compute_rx_o_o, compute_rc_o_i, compute_ry_o_i, compute_rx_o_i, compute_nn_o_i, compute_ff_o_i, compute_yy_o_i, compute_xx_o_i, compute_rc_i, compute_ry_i, compute_rx_i, compute_nn_i, compute_ff_i, compute_yy_i, compute_xx_i)
    compute_i0_o_i, compute_i0_i = s[compute].split(compute_i0, factor=1)
    compute_i0_o_o_i, compute_i0_o_i = s[compute].split(compute_i0_o_i, factor=1)
    compute_i0_o_o_o, compute_i0_o_o_i = s[compute].split(compute_i0_o_o_i, factor=1)
    compute_i1_o_i, compute_i1_i = s[compute].split(compute_i1, factor=2)
    compute_i1_o_o_i, compute_i1_o_i = s[compute].split(compute_i1_o_i, factor=8)
    compute_i1_o_o_o, compute_i1_o_o_i = s[compute].split(compute_i1_o_o_i, factor=2)
    compute_i2_o_i, compute_i2_i = s[compute].split(compute_i2, factor=1)
    compute_i2_o_o_i, compute_i2_o_i = s[compute].split(compute_i2_o_i, factor=1)
    compute_i2_o_o_o, compute_i2_o_o_i = s[compute].split(compute_i2_o_o_i, factor=1)
    compute_i3_o_i, compute_i3_i = s[compute].split(compute_i3, factor=1)
    compute_i3_o_o_i, compute_i3_o_i = s[compute].split(compute_i3_o_i, factor=7)
    compute_i3_o_o_o, compute_i3_o_o_i = s[compute].split(compute_i3_o_o_i, factor=1)
    s[compute].reorder(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o, compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i, compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i, compute_i0_i, compute_i1_i, compute_i2_i, compute_i3_i)
    s[compute].compute_at(s[compute], compute_i3_o_i)
    kernel_shared = s.cache_read(kernel, "shared", [compute])
    kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3 = tuple(kernel_shared.op.axis)
    s[kernel_shared].compute_at(s[compute], compute_rx_o_o)
    pad_temp_shared = s.cache_read(pad_temp, "shared", [compute])
    pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3 = tuple(pad_temp_shared.op.axis)
    s[pad_temp_shared].compute_at(s[compute], compute_rx_o_o)
    s[pad_temp].compute_inline()
    compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused = s[compute].fuse(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o)
    s[compute].bind(compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused, te.thread_axis("blockIdx.x"))
    compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused = s[compute].fuse(compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i)
    s[compute].bind(compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused, te.thread_axis("vthread"))
    compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused = s[compute].fuse(compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i)
    s[compute].bind(compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused, te.thread_axis("threadIdx.x"))
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[kernel_shared].fuse(kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=1)
    s[kernel_shared].vectorize(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=56)
    s[kernel_shared].bind(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[pad_temp_shared].fuse(pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=1)
    s[pad_temp_shared].vectorize(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=56)
    s[pad_temp_shared].bind(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    s[compute].pragma(compute_nn_o_o_o_o, "auto_unroll_max_step", 16)
    s[compute].pragma(compute_nn_o_o_o_o, "unroll_explicit", True)

    CUDA source code:
    extern "C" __global__ void default_function_kernel0(float* __restrict__ data, float* __restrict__ kernel, float* __restrict__ compute, float* __restrict__ bias) {
      float compute1[4];
      __shared__ float pad_temp_shared[54];
      __shared__ float kernel_shared[576];
      compute1[(0)] = 0.000000e+00f;
      compute1[(2)] = 0.000000e+00f;
      compute1[(1)] = 0.000000e+00f;
      compute1[(3)] = 0.000000e+00f;
      for (int rc_outer_outer = 0; rc_outer_outer < 256; ++rc_outer_outer) {
        __syncthreads();
        if (((int)threadIdx.x) < 54) {
          pad_temp_shared[(((int)threadIdx.x))] = (((((1 <= (((((int)threadIdx.x) % 27) / 9) + (((int)blockIdx.x) % 7))) && ((((((int)threadIdx.x) % 27) / 9) + (((int)blockIdx.x) % 7)) < 8)) && (1 <= (((int)threadIdx.x) % 9))) && ((((int)threadIdx.x) % 9) < 8)) ? data[(((((((rc_outer_outer * 98) + ((((int)threadIdx.x) / 27) * 49)) + (((((int)threadIdx.x) % 27) / 9) * 7)) + ((((int)blockIdx.x) % 7) * 7)) + (((int)threadIdx.x) % 9)) - 8))] : 0.000000e+00f);
        }
        kernel_shared[(((int)threadIdx.x))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + ((((int)threadIdx.x) / 18) * 4608)) + (rc_outer_outer * 18)) + (((int)threadIdx.x) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 56))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 56) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 2) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 112))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 112) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 4) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 168))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 168) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 6) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 224))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 224) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 8) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 280))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 280) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 10) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 336))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 336) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 12) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 392))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 392) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 14) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 448))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 448) / 18) * 4608)) + (rc_outer_outer * 18)) + ((((int)threadIdx.x) + 16) % 18)))];
        kernel_shared[((((int)threadIdx.x) + 504))] = kernel[(((((((((int)blockIdx.x) / 7) * 147456) + ((((int)threadIdx.x) / 18) * 4608)) + (rc_outer_outer * 18)) + (((int)threadIdx.x) % 18)) + 129024))];
        if (((int)threadIdx.x) < 16) {
          kernel_shared[((((int)threadIdx.x) + 560))] = kernel[((((((((int)blockIdx.x) / 7) * 147456) + (((((int)threadIdx.x) + 560) / 18) * 4608)) + (rc_outer_outer * 18)) + (((int)threadIdx.x) + 2)))];
        }
        __syncthreads();
        for (int rc_outer_inner = 0; rc_outer_inner < 2; ++rc_outer_inner) {
          for (int rx_outer_inner = 0; rx_outer_inner < 3; ++rx_outer_inner) {
            compute1[(0)] = (compute1[(0)] + (pad_temp_shared[((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)))] * kernel_shared[(((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner))]));
            compute1[(2)] = (compute1[(2)] + (pad_temp_shared[((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 288))]));
            compute1[(0)] = (compute1[(0)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 9))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 3))]));
            compute1[(2)] = (compute1[(2)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 9))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 291))]));
            compute1[(0)] = (compute1[(0)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 18))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 6))]));
            compute1[(2)] = (compute1[(2)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 18))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 294))]));
            compute1[(1)] = (compute1[(1)] + (pad_temp_shared[((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 18))]));
            compute1[(3)] = (compute1[(3)] + (pad_temp_shared[((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 306))]));
            compute1[(1)] = (compute1[(1)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 9))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 21))]));
            compute1[(3)] = (compute1[(3)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 9))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 309))]));
            compute1[(1)] = (compute1[(1)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 18))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 24))]));
            compute1[(3)] = (compute1[(3)] + (pad_temp_shared[(((((rc_outer_inner * 27) + rx_outer_inner) + (((int)threadIdx.x) % 7)) + 18))] * kernel_shared[((((((((int)threadIdx.x) / 7) * 36) + (rc_outer_inner * 9)) + rx_outer_inner) + 312))]));
          }
        }
      }
      for (int i1_inner = 0; i1_inner < 2; ++i1_inner) {
        compute[(((((((((int)blockIdx.x) / 7) * 1568) + ((((int)threadIdx.x) / 7) * 98)) + (i1_inner * 49)) + ((((int)blockIdx.x) % 7) * 7)) + (((int)threadIdx.x) % 7)))] = max((compute1[(i1_inner)] + bias[(((((((int)blockIdx.x) / 7) * 32) + ((((int)threadIdx.x) / 7) * 2)) + i1_inner))]), 0.000000e+00f);
        compute[((((((((((int)blockIdx.x) / 7) * 1568) + ((((int)threadIdx.x) / 7) * 98)) + (i1_inner * 49)) + ((((int)blockIdx.x) % 7) * 7)) + (((int)threadIdx.x) % 7)) + 784))] = max((compute1[((i1_inner + 2))] + bias[((((((((int)blockIdx.x) / 7) * 32) + ((((int)threadIdx.x) / 7) * 2)) + i1_inner) + 16))]), 0.000000e+00f);
      }
    }





A more complicated example is to resume the search.
In this case, we need to create the search policy and cost model by ourselves
and resume the status of search policy and cost model with the log file.
In the example below we resume the status and do more 5 trials.


.. code-block:: default



    def resume_search(task, log_file):
        print("Resume search:")
        cost_model = auto_scheduler.XGBModel()
        cost_model.update_from_file(log_file)
        search_policy = auto_scheduler.SketchPolicy(
            task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]
        )
        measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
        tune_option = auto_scheduler.TuningOptions(
            num_measure_trials=5,
            runner=measure_ctx.runner,
            measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
        )
        task.tune(tune_option, search_policy=search_policy)

        # Kill the measurement process
        del measure_ctx


    resume_search(task, log_file)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Resume search:
    Get devices for measurement successfully!





.. _sphx_glr_download_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: tune_conv2d_layer_cuda.py <tune_conv2d_layer_cuda.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: tune_conv2d_layer_cuda.ipynb <tune_conv2d_layer_cuda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
