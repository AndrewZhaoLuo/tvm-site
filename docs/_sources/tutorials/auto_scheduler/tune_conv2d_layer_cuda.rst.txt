.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py:


.. _auto-scheduler-conv-gpu:

Auto-scheduling a convolution layer for GPU
===========================================
**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_


Different from the existing :ref:`autotvm <tutorials-autotvm-sec>` which relies on 
manual templates to define the search space, the auto-scheduler does not require any templates.
The auto-scheduler is template-free, so users only need to write the computation declaration without
any schedule commands or templates.
The auto-scheduler can automatically generate a large
search space and find a good schedule in the space.

We use a convolution layer as an example in this tutorial.


.. code-block:: default


    import numpy as np
    import tvm
    from tvm import te, auto_scheduler, topi
    from tvm.topi.testing import conv2d_nchw_python







Define the computation
^^^^^^^^^^^^^^^^^^^^^^
To begin with, let us define the computation of a convolution layer.
The function should return the list of input/output tensors.
From these tensors, the auto-scheduler can get the whole computational graph.


.. code-block:: default



    @auto_scheduler.register_workload
    def conv2d_layer(N, H, W, CO, CI, KH, KW, stride, padding):
        data = te.placeholder((N, CI, H, W), name="data")
        kernel = te.placeholder((CO, CI, KH, KW), name="kernel")
        bias = te.placeholder((1, CO, 1, 1), name="bias")
        conv = topi.nn.conv2d_nchw(data, kernel, stride, padding, dilation=1, out_dtype="float32")
        out = topi.nn.relu(conv + bias)
        return [data, kernel, bias, out]








Create the search task
^^^^^^^^^^^^^^^^^^^^^^
We then create a search task for the last convolution layer in the resnet.


.. code-block:: default


    target = tvm.target.Target("cuda")

    # Use the last layer in ResNet-50
    N, H, W, CO, CI, KH, KW, strides, padding = 1, 7, 7, 512, 512, 3, 3, (1, 1), (1, 1)
    task = auto_scheduler.create_task(conv2d_layer, (N, H, W, CO, CI, KH, KW, strides, padding), target)

    # Inspect the computational graph
    print(task.compute_dag)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    data = PLACEHOLDER [1, 512, 7, 7]
    pad_temp(i0, i1, i2, i3) = tir.if_then_else(((((i2 >= 1) && (i2 < 8)) && (i3 >= 1)) && (i3 < 8)), data[i0, i1, (i2 - 1), (i3 - 1)], 0f)
    kernel = PLACEHOLDER [512, 512, 3, 3]
    compute(nn, ff, yy, xx) += (pad_temp[nn, rc, (yy + ry), (xx + rx)]*kernel[ff, rc, ry, rx])
    bias = PLACEHOLDER [1, 512, 1, 1]
    T_add(ax0, ax1, ax2, ax3) = (compute[ax0, ax1, ax2, ax3] + bias[ax0, ax1, 0, 0])
    compute(i0, i1, i2, i3) = max(T_add[i0, i1, i2, i3], 0f)




Next, we set parameters for the auto-scheduler. These parameters
mainly specify how we do the measurement during the search and auto-tuning.

* :code:`measure_ctx` launches a different process for measurement. This
  provides an isolation. It can protect the master process from GPU crashes
  happended during measurement and avoid other runtime conflicts.
* :code:`min_repeat_ms` defines the minimum duration of one "repeat" in every measurement.
  This can warmup the GPU, which is necessary to get accurate measurement results.
  Typically, we recommend a value > 300 ms.
* :code:`num_measure_trials` is the number of measurement trials we can use during the search.
  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a
  good value for the search to converge. You can do more trials according to your time budget.
* In addition, we use :code:`RecordToFile` to dump measurement records into a file `conv2d.json`.
  The measurement records can be used to query the history best, resume the search,
  and do more analyses later.
* see :any:`auto_scheduler.TuningOptions`,
  :any:`auto_scheduler.LocalRPCMeasureContext` for more parameters.


.. code-block:: default


    measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=10,
        runner=measure_ctx.runner,
        measure_callbacks=[auto_scheduler.RecordToFile("conv2d.json")],
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Get devices for measurement successfully!



Run the search
^^^^^^^^^^^^^^
Now we get all inputs ready. Pretty simple, isn't it?
We can kick off the search and let the auto-scheduler do its magic.
After some measurement trials, it will return the best schedule it found.


.. code-block:: default


    sch, args = auto_scheduler.auto_schedule(task, tuning_options=tune_option)

    # Kill the process for measurement
    del measure_ctx





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none





We can lower the schedule to see the IR after auto-scheduling.
The auto-scheduler correctly performs optimizations including multi-level tiling,
cooperative fetching, unrolling and operator fusion.


.. code-block:: default


    print(tvm.lower(sch, args, simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    #[version = "0.0.5"]
    primfn(data_1: handle, kernel_1: handle, bias_1: handle, compute_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {compute: Buffer(compute_2: Pointer(float32), float32, [1, 512, 7, 7], []),
                 kernel: Buffer(kernel_2: Pointer(float32), float32, [512, 512, 3, 3], []),
                 bias: Buffer(bias_2: Pointer(float32), float32, [1, 512, 1, 1], []),
                 data: Buffer(data_2: Pointer(float32), float32, [1, 512, 7, 7], [])}
      buffer_map = {data_1: data, kernel_1: kernel, bias_1: bias, compute_1: compute} {
      attr [IterVar(blockIdx.x: int32, (nullptr), "ThreadIndex", "blockIdx.x")] "thread_extent" = 64;
      attr [compute_3: Pointer(float32)] "storage_scope" = "local";
      allocate(compute_3, float32, [14]);
      attr [pad_temp.shared: Pointer(float32)] "storage_scope" = "shared";
      allocate(pad_temp.shared, float32, [1296]);
      attr [kernel.shared: Pointer(float32)] "storage_scope" = "shared";
      allocate(kernel.shared, float32, [1152]);
      attr [IterVar(threadIdx.x: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
        compute_3[0] = 0f32
        compute_3[1] = 0f32
        compute_3[2] = 0f32
        compute_3[3] = 0f32
        compute_3[4] = 0f32
        compute_3[5] = 0f32
        compute_3[6] = 0f32
        compute_3[7] = 0f32
        compute_3[8] = 0f32
        compute_3[9] = 0f32
        compute_3[10] = 0f32
        compute_3[11] = 0f32
        compute_3[12] = 0f32
        compute_3[13] = 0f32
        for (rc.outer.outer: int32, 0, 32) {
          attr [IterVar(threadIdx.x_1: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[(threadIdx.x_1*2)] = @tir.if_then_else((((5 <= threadIdx.x_1) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[((((rc.outer.outer*784) + (floordiv((threadIdx.x_1*2), 9)*7)) + floormod((threadIdx.x_1*2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1)] = @tir.if_then_else((((4 <= threadIdx.x_1) && (1 <= floormod(((threadIdx.x_1*2) + 1), 9))) && (floormod(((threadIdx.x_1*2) + 1), 9) < 8)), (float32*)data_2[((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1), 9)*7)) + floormod(((threadIdx.x_1*2) + 1), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 56)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 56), 81)) && (floormod(((threadIdx.x_1*2) + 56), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 2), 9))) && (floormod(((threadIdx.x_1*2) + 2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 56), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 56), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 57)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 57), 81)) && (floormod(((threadIdx.x_1*2) + 57), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 3), 9))) && (floormod(((threadIdx.x_1*2) + 3), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 57), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 57), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 3), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 112)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 31), 81)) && (floormod(((threadIdx.x_1*2) + 31), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 4), 9))) && (floormod(((threadIdx.x_1*2) + 4), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 112), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 31), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 4), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 113)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 32), 81)) && (floormod(((threadIdx.x_1*2) + 32), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 5), 9))) && (floormod(((threadIdx.x_1*2) + 5), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 113), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 32), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 5), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 168)] = @tir.if_then_else((((9 <= floormod(((threadIdx.x_1*2) + 6), 81)) && (1 <= floormod(((threadIdx.x_1*2) + 6), 9))) && (floormod(((threadIdx.x_1*2) + 6), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 168), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 6), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 6), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 169)] = @tir.if_then_else((((9 <= floormod(((threadIdx.x_1*2) + 7), 81)) && (1 <= floormod(((threadIdx.x_1*2) + 7), 9))) && (floormod(((threadIdx.x_1*2) + 7), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 169), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 7), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 7), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 224)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 62), 81)) && (floormod(((threadIdx.x_1*2) + 62), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 8), 9))) && (floormod(((threadIdx.x_1*2) + 8), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 224), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 62), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 8), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 225)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 63), 81)) && (floormod(((threadIdx.x_1*2) + 63), 81) < 72)) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 225), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 63), 81), 9)*7)) + floormod((threadIdx.x_1*2), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 280)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 37), 81)) && (floormod(((threadIdx.x_1*2) + 37), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 1), 9))) && (floormod(((threadIdx.x_1*2) + 1), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 280), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 37), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 1), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 281)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 38), 81)) && (floormod(((threadIdx.x_1*2) + 38), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 2), 9))) && (floormod(((threadIdx.x_1*2) + 2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 281), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 38), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 2), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 336)] = @tir.if_then_else(((1 <= floormod(((threadIdx.x_1*2) + 3), 9)) && (floormod(((threadIdx.x_1*2) + 3), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 336), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 12), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 3), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 337)] = @tir.if_then_else(((1 <= floormod(((threadIdx.x_1*2) + 4), 9)) && (floormod(((threadIdx.x_1*2) + 4), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 337), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 13), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 4), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 392)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 68), 81)) && (floormod(((threadIdx.x_1*2) + 68), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 5), 9))) && (floormod(((threadIdx.x_1*2) + 5), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 392), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 68), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 5), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 393)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 69), 81)) && (floormod(((threadIdx.x_1*2) + 69), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 6), 9))) && (floormod(((threadIdx.x_1*2) + 6), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 393), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 69), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 6), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 448)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 43), 81)) && (floormod(((threadIdx.x_1*2) + 43), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 7), 9))) && (floormod(((threadIdx.x_1*2) + 7), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 448), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 43), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 7), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 449)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 44), 81)) && (floormod(((threadIdx.x_1*2) + 44), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 8), 9))) && (floormod(((threadIdx.x_1*2) + 8), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 449), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 44), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 8), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 504)] = @tir.if_then_else((((floormod(((threadIdx.x_1*2) + 18), 81) < 72) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 504), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 18), 81), 9)*7)) + floormod((threadIdx.x_1*2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 505)] = @tir.if_then_else((((floormod(((threadIdx.x_1*2) + 19), 81) < 72) && (1 <= floormod(((threadIdx.x_1*2) + 1), 9))) && (floormod(((threadIdx.x_1*2) + 1), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 505), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 19), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 1), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 560)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 74), 81)) && (floormod(((threadIdx.x_1*2) + 74), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 2), 9))) && (floormod(((threadIdx.x_1*2) + 2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 560), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 74), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 561)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 75), 81)) && (floormod(((threadIdx.x_1*2) + 75), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 3), 9))) && (floormod(((threadIdx.x_1*2) + 3), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 561), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 75), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 3), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 616)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 49), 81)) && (floormod(((threadIdx.x_1*2) + 49), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 4), 9))) && (floormod(((threadIdx.x_1*2) + 4), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 616), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 49), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 4), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 617)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 50), 81)) && (floormod(((threadIdx.x_1*2) + 50), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 5), 9))) && (floormod(((threadIdx.x_1*2) + 5), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 617), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 50), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 5), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 672)] = @tir.if_then_else((((floormod(((threadIdx.x_1*2) + 24), 81) < 72) && (1 <= floormod(((threadIdx.x_1*2) + 6), 9))) && (floormod(((threadIdx.x_1*2) + 6), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 672), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 24), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 6), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 673)] = @tir.if_then_else((((floormod(((threadIdx.x_1*2) + 25), 81) < 72) && (1 <= floormod(((threadIdx.x_1*2) + 7), 9))) && (floormod(((threadIdx.x_1*2) + 7), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 673), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 25), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 7), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 728)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 80), 81)) && (floormod(((threadIdx.x_1*2) + 80), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 8), 9))) && (floormod(((threadIdx.x_1*2) + 8), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 728), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 80), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 8), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 729)] = @tir.if_then_else((((5 <= threadIdx.x_1) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[((((rc.outer.outer*784) + (floordiv((threadIdx.x_1*2), 9)*7)) + floormod((threadIdx.x_1*2), 9)) + 433)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 784)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 55), 81)) && (floormod(((threadIdx.x_1*2) + 55), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 1), 9))) && (floormod(((threadIdx.x_1*2) + 1), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 784), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 55), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 1), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 785)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 56), 81)) && (floormod(((threadIdx.x_1*2) + 56), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 2), 9))) && (floormod(((threadIdx.x_1*2) + 2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 785), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 56), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 2), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 840)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 30), 81)) && (floormod(((threadIdx.x_1*2) + 30), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 3), 9))) && (floormod(((threadIdx.x_1*2) + 3), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 840), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 30), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 3), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 841)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 31), 81)) && (floormod(((threadIdx.x_1*2) + 31), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 4), 9))) && (floormod(((threadIdx.x_1*2) + 4), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 841), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 31), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 4), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 896)] = @tir.if_then_else((((9 <= floormod(((threadIdx.x_1*2) + 5), 81)) && (1 <= floormod(((threadIdx.x_1*2) + 5), 9))) && (floormod(((threadIdx.x_1*2) + 5), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 896), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 5), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 5), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 897)] = @tir.if_then_else((((9 <= floormod(((threadIdx.x_1*2) + 6), 81)) && (1 <= floormod(((threadIdx.x_1*2) + 6), 9))) && (floormod(((threadIdx.x_1*2) + 6), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 897), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 6), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 6), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 952)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 61), 81)) && (floormod(((threadIdx.x_1*2) + 61), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 7), 9))) && (floormod(((threadIdx.x_1*2) + 7), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 952), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 61), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 7), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 953)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 62), 81)) && (floormod(((threadIdx.x_1*2) + 62), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 8), 9))) && (floormod(((threadIdx.x_1*2) + 8), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 953), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 62), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 8), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 1008)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 36), 81)) && (floormod(((threadIdx.x_1*2) + 36), 81) < 72)) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1008), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 36), 81), 9)*7)) + floormod((threadIdx.x_1*2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1009)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 37), 81)) && (floormod(((threadIdx.x_1*2) + 37), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 1), 9))) && (floormod(((threadIdx.x_1*2) + 1), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1009), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 37), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 1), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 1064)] = @tir.if_then_else(((1 <= floormod(((threadIdx.x_1*2) + 2), 9)) && (floormod(((threadIdx.x_1*2) + 2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1064), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 11), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 2), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1065)] = @tir.if_then_else(((1 <= floormod(((threadIdx.x_1*2) + 3), 9)) && (floormod(((threadIdx.x_1*2) + 3), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1065), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 12), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 3), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 1120)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 67), 81)) && (floormod(((threadIdx.x_1*2) + 67), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 4), 9))) && (floormod(((threadIdx.x_1*2) + 4), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1120), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 67), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 4), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1121)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 68), 81)) && (floormod(((threadIdx.x_1*2) + 68), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 5), 9))) && (floormod(((threadIdx.x_1*2) + 5), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1121), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 68), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 5), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 1176)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 42), 81)) && (floormod(((threadIdx.x_1*2) + 42), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 6), 9))) && (floormod(((threadIdx.x_1*2) + 6), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1176), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 42), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 6), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1177)] = @tir.if_then_else(((((9 <= floormod(((threadIdx.x_1*2) + 43), 81)) && (floormod(((threadIdx.x_1*2) + 43), 81) < 72)) && (1 <= floormod(((threadIdx.x_1*2) + 7), 9))) && (floormod(((threadIdx.x_1*2) + 7), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1177), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 43), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 7), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            pad_temp.shared[((threadIdx.x_1*2) + 1232)] = @tir.if_then_else(((1 <= floormod(((threadIdx.x_1*2) + 8), 9)) && (floormod(((threadIdx.x_1*2) + 8), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1232), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 17), 81), 9)*7)) + floormod(((threadIdx.x_1*2) + 8), 9)) - 8)], 0f32, dtype=float32)
            pad_temp.shared[((threadIdx.x_1*2) + 1233)] = @tir.if_then_else((((floormod(((threadIdx.x_1*2) + 18), 81) < 72) && (1 <= floormod((threadIdx.x_1*2), 9))) && (floormod((threadIdx.x_1*2), 9) < 8)), (float32*)data_2[(((((rc.outer.outer*784) + (floordiv(((threadIdx.x_1*2) + 1233), 81)*49)) + (floordiv(floormod(((threadIdx.x_1*2) + 18), 81), 9)*7)) + floormod((threadIdx.x_1*2), 9)) - 8)], 0f32, dtype=float32)
          }
          attr [IterVar(threadIdx.x_1, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            if @tir.likely((threadIdx.x_1 < 4), dtype=bool) {
              pad_temp.shared[((threadIdx.x_1*2) + 1288)] = 0f32
            }
            if @tir.likely(((threadIdx.x_1*2) < 7), dtype=bool) {
              if @tir.likely((threadIdx.x_1 < 4), dtype=bool) {
                pad_temp.shared[((threadIdx.x_1*2) + 1289)] = 0f32
              }
            }
          }
          attr [IterVar(threadIdx.x_2: int32, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[(threadIdx.x_2*2)] = (float32*)kernel_2[(((blockIdx.x*36864) + (rc.outer.outer*144)) + (threadIdx.x_2*2))]
            kernel.shared[((threadIdx.x_2*2) + 1)] = (float32*)kernel_2[(((blockIdx.x*36864) + (rc.outer.outer*144)) + ((threadIdx.x_2*2) + 1))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 56)] = (float32*)kernel_2[(((blockIdx.x*36864) + (rc.outer.outer*144)) + ((threadIdx.x_2*2) + 56))]
            kernel.shared[((threadIdx.x_2*2) + 57)] = (float32*)kernel_2[(((blockIdx.x*36864) + (rc.outer.outer*144)) + ((threadIdx.x_2*2) + 57))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 112)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 112), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 112), 144))]
            kernel.shared[((threadIdx.x_2*2) + 113)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 113), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 113), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 168)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 168), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 24), 144))]
            kernel.shared[((threadIdx.x_2*2) + 169)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 169), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 25), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 224)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 224), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 80), 144))]
            kernel.shared[((threadIdx.x_2*2) + 225)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 225), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 81), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 280)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 280), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 136), 144))]
            kernel.shared[((threadIdx.x_2*2) + 281)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 281), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 137), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 336)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 336), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 48), 144))]
            kernel.shared[((threadIdx.x_2*2) + 337)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 337), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 49), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 392)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 392), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 104), 144))]
            kernel.shared[((threadIdx.x_2*2) + 393)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 393), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 105), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 448)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 448), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 16), 144))]
            kernel.shared[((threadIdx.x_2*2) + 449)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 449), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 17), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 504)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 504), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 72), 144))]
            kernel.shared[((threadIdx.x_2*2) + 505)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 505), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 73), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 560)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 560), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 128), 144))]
            kernel.shared[((threadIdx.x_2*2) + 561)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 561), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 129), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 616)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 616), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 40), 144))]
            kernel.shared[((threadIdx.x_2*2) + 617)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 617), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 41), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 672)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 672), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 96), 144))]
            kernel.shared[((threadIdx.x_2*2) + 673)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 673), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 97), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 728)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 728), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 8), 144))]
            kernel.shared[((threadIdx.x_2*2) + 729)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 729), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 9), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 784)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 784), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 64), 144))]
            kernel.shared[((threadIdx.x_2*2) + 785)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 785), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 65), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 840)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 840), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 120), 144))]
            kernel.shared[((threadIdx.x_2*2) + 841)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 841), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 121), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 896)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 896), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 32), 144))]
            kernel.shared[((threadIdx.x_2*2) + 897)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 897), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 33), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 952)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 952), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 88), 144))]
            kernel.shared[((threadIdx.x_2*2) + 953)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 953), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 89), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 1008)] = (float32*)kernel_2[((((blockIdx.x*36864) + (rc.outer.outer*144)) + (threadIdx.x_2*2)) + 32256)]
            kernel.shared[((threadIdx.x_2*2) + 1009)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 1009), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 1), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            kernel.shared[((threadIdx.x_2*2) + 1064)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 1064), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 56), 144))]
            kernel.shared[((threadIdx.x_2*2) + 1065)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 1065), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 57), 144))]
          }
          attr [IterVar(threadIdx.x_2, (nullptr), "ThreadIndex", "threadIdx.x")] "thread_extent" = 28 {
            if @tir.likely((threadIdx.x_2 < 16), dtype=bool) {
              kernel.shared[((threadIdx.x_2*2) + 1120)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 1120), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 112), 144))]
            }
            if @tir.likely(((threadIdx.x_2*2) < 31), dtype=bool) {
              if @tir.likely((threadIdx.x_2 < 16), dtype=bool) {
                kernel.shared[((threadIdx.x_2*2) + 1121)] = (float32*)kernel_2[((((blockIdx.x*36864) + (floordiv(((threadIdx.x_2*2) + 1121), 144)*4608)) + (rc.outer.outer*144)) + floormod(((threadIdx.x_2*2) + 113), 144))]
              }
            }
          }
          for (rc.outer.inner: int32, 0, 16) {
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9))]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 1)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9))]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9))]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 1)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 144)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 1)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 7)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 1)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 1)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 7)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 145)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 7)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 8)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 2)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 2)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 3)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 4)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 5)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 6)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 7)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 8)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 146)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 9)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 10)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 3)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 9)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 10)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 147)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 10)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 16)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 4)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 10)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 16)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 148)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 16)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 17)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 5)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 11)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 12)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 13)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 14)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 15)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 16)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 17)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 149)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 18)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 19)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 6)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 18)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 19)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 150)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 19)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 25)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 7)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 19)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 25)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 151)]))
            compute_3[0] = ((float32*)compute_3[0] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[1] = ((float32*)compute_3[1] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[2] = ((float32*)compute_3[2] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[3] = ((float32*)compute_3[3] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[4] = ((float32*)compute_3[4] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[5] = ((float32*)compute_3[5] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 25)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[6] = ((float32*)compute_3[6] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 26)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 8)]))
            compute_3[7] = ((float32*)compute_3[7] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 20)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[8] = ((float32*)compute_3[8] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 21)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[9] = ((float32*)compute_3[9] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 22)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[10] = ((float32*)compute_3[10] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 23)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[11] = ((float32*)compute_3[11] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 24)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[12] = ((float32*)compute_3[12] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 25)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
            compute_3[13] = ((float32*)compute_3[13] + ((float32*)pad_temp.shared[(((rc.outer.inner*81) + (floormod(threadIdx.x, 7)*9)) + 26)]*(float32*)kernel.shared[(((floordiv(threadIdx.x, 7)*288) + (rc.outer.inner*9)) + 152)]))
          }
        }
        for (i1.inner: int32, 0, 2) {
          for (i3.inner: int32, 0, 7) {
            compute_2[(((((blockIdx.x*392) + (floordiv(threadIdx.x, 7)*98)) + (i1.inner*49)) + (floormod(threadIdx.x, 7)*7)) + i3.inner)] = max(((float32*)compute_3[((i1.inner*7) + i3.inner)] + (float32*)bias_2[(((blockIdx.x*8) + (floordiv(threadIdx.x, 7)*2)) + i1.inner)]), 0f32)
          }
        }
      }
    }

    #[metadata]
    {
      "root": 1, 
      "nodes": [
        {
          "type_key": ""
        }, 
        {
          "type_key": "Map", 
          "keys": [
            "IntImm"
          ], 
          "data": [2]
        }, 
        {
          "type_key": "Array", 
          "data": [3]
        }, 
        {
          "type_key": "IntImm", 
          "attrs": {
            "dtype": "bool", 
            "value": "1"
          }
        }
      ], 
      "b64ndarrays": [], 
      "attrs": {"tvm_version": "0.8.dev0"}
    }



Check correctness and evaluate performance
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
We build the binary and check its correctness and performance.


.. code-block:: default


    func = tvm.build(sch, args, target)

    # Check correctness
    data_np = np.random.uniform(size=(N, CI, H, W)).astype(np.float32)
    weight_np = np.random.uniform(size=(CO, CI, KH, KW)).astype(np.float32)
    bias_np = np.random.uniform(size=(1, CO, 1, 1)).astype(np.float32)
    conv_np = conv2d_nchw_python(data_np, weight_np, strides, padding)
    out_np = np.maximum(conv_np + bias_np, 0.0)

    ctx = tvm.gpu()
    data_tvm = tvm.nd.array(data_np, ctx=ctx)
    weight_tvm = tvm.nd.array(weight_np, ctx=ctx)
    bias_tvm = tvm.nd.array(bias_np, ctx=ctx)
    out_tvm = tvm.nd.empty(out_np.shape, ctx=ctx)
    func(data_tvm, weight_tvm, bias_tvm, out_tvm)

    # Check results
    np.testing.assert_allclose(out_np, out_tvm.asnumpy(), rtol=1e-3)

    # Evaluate execution time
    evaluator = func.time_evaluator(func.entry_name, ctx, min_repeat_ms=500)
    print(
        "Execution time of this operator: %.3f ms"
        % (np.median(evaluator(data_tvm, weight_tvm, bias_tvm, out_tvm).results) * 1000)
    )





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Execution time of this operator: 0.173 ms



Using the record file
^^^^^^^^^^^^^^^^^^^^^
During the search, all measuremnt records are dumpped into the record
file "conv2d.json". The measurement records can be used to re-apply search results,
resume the search, and perform other analyses.

Here is an example where we load the best schedule from a file,
print the equivalent python schedule API, and build the binary again.


.. code-block:: default


    # Load the measuremnt record for the best schedule
    inp, res = auto_scheduler.load_best("conv2d.json", task.workload_key)

    # Print equivalent python schedule API. This can be used for debugging and
    # learning the behavior of the auto-scheduler.
    print("Equivalent python schedule:")
    print(task.compute_dag.print_python_code_from_state(inp.state))

    # Rebuild the binary. This shows how you can apply the best schedule from a
    # log file without reruning the search again.
    sch, args = task.compute_dag.apply_steps_from_state(inp.state)
    func = tvm.build(sch, args, target)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Equivalent python schedule:
    pad_temp_i0, pad_temp_i1, pad_temp_i2, pad_temp_i3 = tuple(pad_temp.op.axis) + tuple(pad_temp.op.reduce_axis)
    compute_nn, compute_ff, compute_yy, compute_xx, compute_rc, compute_ry, compute_rx = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
    T_add_ax0, T_add_ax1, T_add_ax2, T_add_ax3 = tuple(T_add.op.axis) + tuple(T_add.op.reduce_axis)
    compute_i0, compute_i1, compute_i2, compute_i3 = tuple(compute.op.axis) + tuple(compute.op.reduce_axis)
    s[T_add].compute_inline()
    compute_nn_o_i, compute_nn_i = s[compute].split(compute_nn, factor=1)
    compute_nn_o_o_i, compute_nn_o_i = s[compute].split(compute_nn_o_i, factor=1)
    compute_nn_o_o_o_i, compute_nn_o_o_i = s[compute].split(compute_nn_o_o_i, factor=1)
    compute_nn_o_o_o_o, compute_nn_o_o_o_i = s[compute].split(compute_nn_o_o_o_i, factor=1)
    compute_ff_o_i, compute_ff_i = s[compute].split(compute_ff, factor=1)
    compute_ff_o_o_i, compute_ff_o_i = s[compute].split(compute_ff_o_i, factor=1)
    compute_ff_o_o_o_i, compute_ff_o_o_i = s[compute].split(compute_ff_o_o_i, factor=16)
    compute_ff_o_o_o_o, compute_ff_o_o_o_i = s[compute].split(compute_ff_o_o_o_i, factor=1)
    compute_yy_o_i, compute_yy_i = s[compute].split(compute_yy, factor=1)
    compute_yy_o_o_i, compute_yy_o_i = s[compute].split(compute_yy_o_i, factor=1)
    compute_yy_o_o_o_i, compute_yy_o_o_i = s[compute].split(compute_yy_o_o_i, factor=7)
    compute_yy_o_o_o_o, compute_yy_o_o_o_i = s[compute].split(compute_yy_o_o_o_i, factor=1)
    compute_xx_o_i, compute_xx_i = s[compute].split(compute_xx, factor=1)
    compute_xx_o_o_i, compute_xx_o_i = s[compute].split(compute_xx_o_i, factor=7)
    compute_xx_o_o_o_i, compute_xx_o_o_i = s[compute].split(compute_xx_o_o_i, factor=1)
    compute_xx_o_o_o_o, compute_xx_o_o_o_i = s[compute].split(compute_xx_o_o_o_i, factor=1)
    compute_rc_o_i, compute_rc_i = s[compute].split(compute_rc, factor=4)
    compute_rc_o_o, compute_rc_o_i = s[compute].split(compute_rc_o_i, factor=4)
    compute_ry_o_i, compute_ry_i = s[compute].split(compute_ry, factor=3)
    compute_ry_o_o, compute_ry_o_i = s[compute].split(compute_ry_o_i, factor=1)
    compute_rx_o_i, compute_rx_i = s[compute].split(compute_rx, factor=3)
    compute_rx_o_o, compute_rx_o_i = s[compute].split(compute_rx_o_i, factor=1)
    s[compute].reorder(compute_nn_o_o_o_o, compute_ff_o_o_o_o, compute_yy_o_o_o_o, compute_xx_o_o_o_o, compute_nn_o_o_o_i, compute_ff_o_o_o_i, compute_yy_o_o_o_i, compute_xx_o_o_o_i, compute_nn_o_o_i, compute_ff_o_o_i, compute_yy_o_o_i, compute_xx_o_o_i, compute_rc_o_o, compute_ry_o_o, compute_rx_o_o, compute_rc_o_i, compute_ry_o_i, compute_rx_o_i, compute_nn_o_i, compute_ff_o_i, compute_yy_o_i, compute_xx_o_i, compute_rc_i, compute_ry_i, compute_rx_i, compute_nn_i, compute_ff_i, compute_yy_i, compute_xx_i)
    compute_i0_o_i, compute_i0_i = s[compute].split(compute_i0, factor=1)
    compute_i0_o_o_i, compute_i0_o_i = s[compute].split(compute_i0_o_i, factor=1)
    compute_i0_o_o_o, compute_i0_o_o_i = s[compute].split(compute_i0_o_o_i, factor=1)
    compute_i1_o_i, compute_i1_i = s[compute].split(compute_i1, factor=1)
    compute_i1_o_o_i, compute_i1_o_i = s[compute].split(compute_i1_o_i, factor=16)
    compute_i1_o_o_o, compute_i1_o_o_i = s[compute].split(compute_i1_o_o_i, factor=1)
    compute_i2_o_i, compute_i2_i = s[compute].split(compute_i2, factor=1)
    compute_i2_o_o_i, compute_i2_o_i = s[compute].split(compute_i2_o_i, factor=7)
    compute_i2_o_o_o, compute_i2_o_o_i = s[compute].split(compute_i2_o_o_i, factor=1)
    compute_i3_o_i, compute_i3_i = s[compute].split(compute_i3, factor=7)
    compute_i3_o_o_i, compute_i3_o_i = s[compute].split(compute_i3_o_i, factor=1)
    compute_i3_o_o_o, compute_i3_o_o_i = s[compute].split(compute_i3_o_o_i, factor=1)
    s[compute].reorder(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o, compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i, compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i, compute_i0_i, compute_i1_i, compute_i2_i, compute_i3_i)
    s[compute].compute_at(s[compute], compute_i3_o_i)
    kernel_shared = s.cache_read(kernel, "shared", [compute])
    kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3 = tuple(kernel_shared.op.axis)
    s[kernel_shared].compute_at(s[compute], compute_rx_o_o)
    pad_temp_shared = s.cache_read(pad_temp, "shared", [compute])
    pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3 = tuple(pad_temp_shared.op.axis)
    s[pad_temp_shared].compute_at(s[compute], compute_rx_o_o)
    s[pad_temp].compute_inline()
    compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused = s[compute].fuse(compute_i0_o_o_o, compute_i1_o_o_o, compute_i2_o_o_o, compute_i3_o_o_o)
    s[compute].bind(compute_i0_o_o_o_i1_o_o_o_fused_i2_o_o_o_fused_i3_o_o_o_fused, te.thread_axis("blockIdx.x"))
    compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused = s[compute].fuse(compute_i0_o_o_i, compute_i1_o_o_i, compute_i2_o_o_i, compute_i3_o_o_i)
    s[compute].bind(compute_i0_o_o_i_i1_o_o_i_fused_i2_o_o_i_fused_i3_o_o_i_fused, te.thread_axis("vthread"))
    compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused = s[compute].fuse(compute_i0_o_i, compute_i1_o_i, compute_i2_o_i, compute_i3_o_i)
    s[compute].bind(compute_i0_o_i_i1_o_i_fused_i2_o_i_fused_i3_o_i_fused, te.thread_axis("threadIdx.x"))
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[kernel_shared].fuse(kernel_shared_ax0, kernel_shared_ax1, kernel_shared_ax2, kernel_shared_ax3)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=3)
    s[kernel_shared].vectorize(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[kernel_shared].split(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=112)
    s[kernel_shared].bind(kernel_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused = s[pad_temp_shared].fuse(pad_temp_shared_ax0, pad_temp_shared_ax1, pad_temp_shared_ax2, pad_temp_shared_ax3)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused, factor=9)
    s[pad_temp_shared].vectorize(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_i)
    pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_o, pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i = s[pad_temp_shared].split(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o, factor=112)
    s[pad_temp_shared].bind(pad_temp_shared_ax0_ax1_fused_ax2_fused_ax3_fused_o_i, te.thread_axis("threadIdx.x"))
    s[compute].pragma(compute_nn_o_o_o_o, "auto_unroll_max_step", 1024)
    s[compute].pragma(compute_nn_o_o_o_o, "unroll_explicit", True)




A more complicated example is to resume the search.
In this case, we need to create the search policy and cost model by ourselves
and resume the status of search policy and cost model with the log file.
In the example below we resume the status and do more 5 trials.


.. code-block:: default



    log_file = "conv2d.json"
    cost_model = auto_scheduler.XGBModel()
    cost_model.update_from_file(log_file)
    search_policy = auto_scheduler.SketchPolicy(
        task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]
    )
    measure_ctx = auto_scheduler.LocalRPCMeasureContext(min_repeat_ms=300)
    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=5,
        runner=measure_ctx.runner,
        measure_callbacks=[auto_scheduler.RecordToFile(log_file)],
    )
    sch, args = auto_scheduler.auto_schedule(task, search_policy, tuning_options=tune_option)

    # Kill the measurement process
    del measure_ctx




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Get devices for measurement successfully!





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  4.894 seconds)


.. _sphx_glr_download_tutorials_auto_scheduler_tune_conv2d_layer_cuda.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: tune_conv2d_layer_cuda.py <tune_conv2d_layer_cuda.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: tune_conv2d_layer_cuda.ipynb <tune_conv2d_layer_cuda.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
