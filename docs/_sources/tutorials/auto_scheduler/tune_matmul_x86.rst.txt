.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_tutorials_auto_scheduler_tune_matmul_x86.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_tutorials_auto_scheduler_tune_matmul_x86.py:


Auto-scheduling matrix multiplication for CPU
=============================================
**Author**: `Lianmin Zheng <https://github.com/merrymercy>`_,             `Chengfan Jia <https://github.com/jcf94/>`_

Different from the existing :ref:`autotvm <tutorials-autotvm-sec>` which relies on 
manual templates to define the search space, the auto-scheduler does not require any templates.
The auto-scheduler is template-free, so users only need to write the computation declaration without
any schedule commands or templates.
The auto-scheduler can automatically generate a large
search space and find a good schedule in the space.

We use matrix multiplication as an example in this tutorial.


.. code-block:: default


    import numpy as np
    import tvm
    from tvm import te, testing, auto_scheduler







Define the computation
^^^^^^^^^^^^^^^^^^^^^^
To begin with, we define the computation of a matmul with bias add.
The function should return the list of input/output tensors.
From these tensors, the auto-scheduler can get the whole computational graph.


.. code-block:: default



    @auto_scheduler.register_workload
    def matmul_add(N, L, M, dtype):
        A = te.placeholder((N, L), name="A", dtype=dtype)
        B = te.placeholder((L, M), name="B", dtype=dtype)
        C = te.placeholder((N, M), name="C", dtype=dtype)

        k = te.reduce_axis((0, L), name="k")
        matmul = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name="matmul")
        out = te.compute((N, M), lambda i, j: matmul[i, j] + C[i, j], name="out")

        return [A, B, C, out]








Create the search task
^^^^^^^^^^^^^^^^^^^^^^
We then create a search task with N=L=M=128 and dtype="float32"


.. code-block:: default


    target = tvm.target.Target("llvm")
    task = auto_scheduler.create_task(matmul_add, (128, 128, 128, "float32"), target)

    # Inspect the computational graph
    print(task.compute_dag)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    A = PLACEHOLDER [128, 128]
    B = PLACEHOLDER [128, 128]
    matmul(i, j) += (A[i, k]*B[k, j])
    C = PLACEHOLDER [128, 128]
    out(i, j) = (matmul[i, j] + C[i, j])




Next, we set parameters for the auto-scheduler.

* `num_measure_trials` is the number of measurement trials we can use during the search.
  We only make 10 trials in this tutorial for a fast demonstration. In practice, 1000 is a
  good value for the search to converge. You can do more trials according to your time budget.
* In addition, we use `RecordToFile` to dump measurement records into a file `matmul.json`.
  The measurement records can be used to query the history best, resume the search,
  and do more analyses later.
* see :any:`auto_schedule.TuningOptions`: for more parameters


.. code-block:: default


    tune_option = auto_scheduler.TuningOptions(
        num_measure_trials=10, measure_callbacks=[auto_scheduler.RecordToFile("matmul.json")]
    )







Run the search
^^^^^^^^^^^^^^
Now we get all inputs ready. Pretty simple, isn't it?
We can kick off the search and let the auto-scheduler do its magic.
After some measurement trials, it will return the best schedule it found.


.. code-block:: default


    sch, args = auto_scheduler.auto_schedule(task, tuning_options=tune_option)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    *T*T*T*T*T*T*T*T



We can lower the schedule to see the IR after auto-scheduling.
The auto-scheduler correctly performs optimizations including multi-level tiling,
parallelization, vectorization, unrolling and fusion.


.. code-block:: default


    print(tvm.lower(sch, args, simple_mode=True))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    primfn(A_1: handle, B_1: handle, C_1: handle, out_1: handle) -> ()
      attr = {"global_symbol": "main", "tir.noalias": True}
      buffers = {C: Buffer(C_2: Pointer(float32), float32, [128, 128], []),
                 out: Buffer(out_2: Pointer(float32), float32, [128, 128], []),
                 B: Buffer(B_2: Pointer(float32), float32, [128, 128], []),
                 A: Buffer(A_2: Pointer(float32), float32, [128, 128], [])}
      buffer_map = {A_1: A, B_1: B, C_1: C, out_1: out} {
      attr [matmul: Pointer(float32)] "storage_scope" = "global";
      allocate(matmul, float32, [2048]);
      for (j.outer: int32, 0, 8) {
        for (i.outer.outer.inner: int32, 0, 64) {
          for (j.outer.outer.inner: int32, 0, 2) {
            matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] = broadcast(0f32, 8)
            matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] = broadcast(0f32, 8)
            for (k.outer: int32, 0, 32) {
              matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] = ((float32x8*)matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] + (broadcast((float32*)A_2[((i.outer.outer.inner*256) + (k.outer*4))], 8)*(float32x8*)B_2[ramp((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)), 1, 8)]))
              matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] = ((float32x8*)matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 128)], 8)*(float32x8*)B_2[ramp((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)), 1, 8)]))
              matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] = ((float32x8*)matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 1)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 128), 1, 8)]))
              matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] = ((float32x8*)matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 129)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 128), 1, 8)]))
              matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] = ((float32x8*)matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 2)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 256), 1, 8)]))
              matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] = ((float32x8*)matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 130)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 256), 1, 8)]))
              matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] = ((float32x8*)matmul[ramp(((i.outer.outer.inner*32) + (j.outer.outer.inner*8)), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 3)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 384), 1, 8)]))
              matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] = ((float32x8*)matmul[ramp((((i.outer.outer.inner*32) + (j.outer.outer.inner*8)) + 16), 1, 8)] + (broadcast((float32*)A_2[(((i.outer.outer.inner*256) + (k.outer*4)) + 131)], 8)*(float32x8*)B_2[ramp(((((k.outer*512) + (j.outer*16)) + (j.outer.outer.inner*8)) + 384), 1, 8)]))
            }
          }
        }
        for (i.inner: int32, 0, 128) {
          out_2[ramp(((i.inner*128) + (j.outer*16)), 1, 16)] = ((float32x16*)matmul[ramp((i.inner*16), 1, 16)] + (float32x16*)C_2[ramp(((i.inner*128) + (j.outer*16)), 1, 16)])
        }
      }
    }





Check correctness
^^^^^^^^^^^^^^^^^
We build the binary and check its correctness


.. code-block:: default


    func = tvm.build(sch, args)
    a_np = np.random.uniform(size=(128, 128)).astype(np.float32)
    b_np = np.random.uniform(size=(128, 128)).astype(np.float32)
    c_np = np.random.uniform(size=(128, 128)).astype(np.float32)
    d_np = a_np.dot(b_np) + c_np

    d_tvm = tvm.nd.empty(d_np.shape)
    func(tvm.nd.array(a_np), tvm.nd.array(b_np), tvm.nd.array(c_np), d_tvm)

    tvm.testing.assert_allclose(d_np, d_tvm.asnumpy(), rtol=1e-3)







Using the record file
^^^^^^^^^^^^^^^^^^^^^
During the search, all measuremnt records are dumpped into the record
file "matmul.json". The measurement records can be used to re-apply search results,
resume the search, and perform other analyses.

Here is an example where we load the best schedule from a file,
print the equivalent python schedule API, and build the binary again.


.. code-block:: default


    # Load the measuremnt record for the best schedule
    inp, res = auto_scheduler.load_best("matmul.json", task.workload_key)

    # Print equivalent python schedule API. This can be used for debugging and
    # learning the behavior of the auto-scheduler.
    print(task.compute_dag.print_python_code_from_state(inp.state))

    # Rebuild the binary. This shows how you can apply the best schedule from a
    # log file without reruning the search again.
    sch, args = task.compute_dag.apply_steps_from_state(inp.state)
    func = tvm.build(sch, args)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    i, j, k = tuple(matmul.op.axis) + tuple(matmul.op.reduce_axis)
    i, j = tuple(out.op.axis) + tuple(out.op.reduce_axis)
    i_o_i, i_i = s[matmul].split(i, factor=1)
    i_o_o_i, i_o_i = s[matmul].split(i_o_i, factor=32)
    i_o_o_o, i_o_o_i = s[matmul].split(i_o_o_i, factor=4)
    j_o_i, j_i = s[matmul].split(j, factor=8)
    j_o_o_i, j_o_i = s[matmul].split(j_o_i, factor=4)
    j_o_o_o, j_o_o_i = s[matmul].split(j_o_o_i, factor=4)
    k_o, k_i = s[matmul].split(k, factor=2)
    s[matmul].reorder(i_o_o_o, j_o_o_o, i_o_o_i, j_o_o_i, k_o, i_o_i, j_o_i, k_i, i_i, j_i)
    i_o, i_i = s[out].split(i, factor=128)
    j_o, j_i = s[out].split(j, factor=128)
    s[out].reorder(i_o, j_o, i_i, j_i)
    s[matmul].compute_at(s[out], j_o)
    i_o = s[out].fuse(i_o)
    s[out].parallel(i_o)
    s[matmul].pragma(i_o_o_o, "auto_unroll_max_step", 0)
    s[matmul].pragma(i_o_o_o, "unroll_explicit", True)
    s[matmul].vectorize(j_i)




A more complicated example is to resume the search.
In this case, we need to create the search policy and cost model by ourselves
and resume the status of search policy and cost model with the log file.
In the example below we resume the status and do more 5 trials.


.. code-block:: default



    def resume_search(task, log_file):
        cost_model = auto_scheduler.XGBModel()
        cost_model.update_from_file(log_file)
        search_policy = auto_scheduler.SketchPolicy(
            task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]
        )
        tune_option = auto_scheduler.TuningOptions(
            num_measure_trials=5, measure_callbacks=[auto_scheduler.RecordToFile(log_file)]
        )
        sch, args = auto_scheduler.auto_schedule(task, search_policy, tuning_options=tune_option)


    # resume_search(task, "matmul.json")







.. note::
  We cannot run the line above because of the conflict between
  python's multiprocessing and tvm's thread pool.
  After running a tvm generated binary (L112), the python's multiprocessing
  library will hang forever.
  You have to make sure that you don't run any tvm generated binaries before
  calling ansor's search. To run the L156 above, you should comment out L112-114.

  You should be careful about this problem in your applications.
  There are other workarounds for this problem.
  For example, you can start a new thread/process (with the builtin python library
  threading or multiprocessing) and run the tvm binaries in the new thread/process.
  This provides an isolation and avoids the conflict in the main thread/process.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  31.257 seconds)


.. _sphx_glr_download_tutorials_auto_scheduler_tune_matmul_x86.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: tune_matmul_x86.py <tune_matmul_x86.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: tune_matmul_x86.ipynb <tune_matmul_x86.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
